# Adobe Hackathon 2025 - Round 1B Solution

## ğŸ§  Persona-Driven Document Intelligence

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://docker.com)
[![Platform](https://img.shields.io/badge/Platform-linux%2Famd64-green.svg)](https://docs.docker.com/desktop/multi-arch/)

> **ğŸ† COMPLETE ROUND 1B SOLUTION**  
> **Model Size**: 128MB (well under 1GB limit âœ…)  
> **Processing Time**: <10 seconds (under 60s limit âœ…)  
> **Offline Operation**: Complete independence âœ…

## ğŸ“‹ Overview

Advanced semantic document analysis system that intelligently extracts the most relevant sections from PDF collections based on user persona and specific job requirements. The solution uses state-of-the-art embedding models to understand context and relevance, delivering ranked results that are tailored to specific use cases.

## âœ¨ Key Features

- **ğŸ§  Semantic Intelligence**: Uses e5-small-v2 embeddings for deep contextual understanding
- **ğŸ‘¤ Model-Based Persona Analysis**: No hardcoded rules - uses AI model for dynamic persona adaptation
- **ğŸ“¦ Complete Offline Operation**: No internet access required during execution
- **âš¡ High Performance**: Processes document collections in seconds
- **ğŸ³ Docker Containerized**: Fully isolated, reproducible environment
- **ğŸ“Š Structured Output**: Adobe-compliant JSON format with metadata

## ğŸš€ Complete Execution Process

### Prerequisites
- Docker Desktop with Linux containers support
- Minimum 4GB RAM available
- ~2GB free disk space

Hereâ€™s your updated `README.md` with a section added to help reviewers clone your repo **with Git LFS**, and to explain what happens if they download the ZIP instead. Iâ€™ve added this right after the **Prerequisites** section:

---

## ğŸš€ Complete Execution Process

### Prerequisites

* Docker Desktop with Linux containers support
* Minimum 4GB RAM available
* \~2GB free disk space

---

### âš ï¸ Important: How to Clone with LFS

> This project includes large files (models, wheel packages) managed using **Git Large File Storage (LFS)**.
> Downloading the ZIP from GitHub will **NOT include these files**.

#### âœ… To clone properly:

Make sure you have [Git LFS installed](https://git-lfs.com), then run:

```bash
git lfs install
git clone https://github.com/Prem-Kumar-Dev/AIHROUND1B.git
cd AIHROUND1B
```

If you've already cloned it without LFS:

```bash
git lfs pull
```

---
### Step 1: Build the Docker Image
```bash
# Build the optimized Docker container
docker build --platform linux/amd64 -t pdf-analyzer .
```

### Step 2: Prepare Your Input Data
Create your input structure with the following format:
```
input/
â”œâ”€â”€ input_config.json          # Configuration with persona and job
â””â”€â”€ PDFs/                      # Your PDF documents
    â”œâ”€â”€ document1.pdf
    â”œâ”€â”€ document2.pdf
    â””â”€â”€ document3.pdf
```

### Step 3: Create Configuration File
Create `input/input_config.json` with your requirements:

```json
{
  "metadata": {
    "input_documents": [
      "document1.pdf",
      "document2.pdf", 
      "document3.pdf"
    ],
    "persona": "Travel Planner",
    "job_to_be_done": "Plan a 4-day vacation for college friends",
    "processing_timestamp": "2025-07-29T10:00:00Z"
  }
}
```

### Step 4: Execute Round 1B Analysis
```bash
# Run the semantic analysis (Linux/macOS)
docker run --rm \
  -v "$(pwd)/input:/app/input" \
  -v "$(pwd)/output:/app/output" \
  --network none \
  pdf-analyzer
```

**Windows PowerShell:**
```powershell
# Run the analysis on Windows
docker run --rm -v "${PWD}/input:/app/input" -v "${PWD}/output:/app/output" --network none pdf-analyzer
```

### Step 5: Review Results
Check the generated output:
```bash
# View the results
cat output/challenge1b_output.json

# Or on Windows
type output\challenge1b_output.json
```

## ğŸ§  AI-Powered Persona Intelligence (No Hardcoding!)

### Dynamic Model-Based Analysis
Our solution **does NOT use hardcoded persona detection**. Instead, it employs:

- **ğŸ¤– e5-small-v2 Embedding Model**: Advanced transformer model that understands semantic relationships
- **ğŸ“Š Dynamic Similarity Scoring**: Real-time analysis of persona-job combinations
- **ğŸ”„ Adaptive Context Generation**: Model generates persona-specific query contexts on-the-fly
- **ğŸ¯ Intelligent Ranking**: AI-driven relevance scoring without predetermined rules

### How the AI Model Works
```python
# Example of model-based persona processing (simplified)
def generate_persona_context(persona, job_to_be_done):
    """AI model dynamically generates context - no hardcoding!"""
    # Model understands persona semantically
    persona_embedding = model.encode(f"query: {persona}")
    job_embedding = model.encode(f"query: {job_to_be_done}")
    
    # Dynamic composite query generation
    composite_query = f"query: {persona} {job_to_be_done}"
    return model.encode(composite_query)
```

### Supported Persona Types
The AI model can dynamically adapt to ANY persona, including but not limited to:
- **Travel Planner**: Tourism, destinations, activities, accommodations
- **HR Professional**: Recruitment, skills assessment, training materials
- **Home Cook**: Recipes, ingredients, cooking techniques, meal planning
- **Student**: Academic content, learning materials, study guides
- **Business Analyst**: Data insights, market research, strategic planning
- **And more...** - The model adapts to any persona you specify!

## ğŸ“Š Output Format

The solution generates `output/challenge1b_output.json`:

```json
{
  "metadata": {
    "input_documents": ["doc1.pdf", "doc2.pdf"],
    "persona": "Travel Planner", 
    "job_to_be_done": "Plan a vacation",
    "processing_timestamp": "2025-07-29T10:15:30.123456Z"
  },
  "extracted_sections": [
    {
      "document": "document1.pdf",
      "section_title": "Top Destinations",
      "importance_rank": 1,
      "page_number": 3
    },
    {
      "document": "document2.pdf", 
      "section_title": "Travel Tips",
      "importance_rank": 2,
      "page_number": 7
    }
    // ... up to 20 most relevant sections
  ]
}
```

## ğŸ§ª Test with Sample Data

The project includes sample South of France travel documents for testing:

```bash
# Test with included sample data
docker run --rm \
  -v "$(pwd)/input:/app/input" \
  -v "$(pwd)/output:/app/output" \
  --network none \
  pdf-analyzer

# Check results
cat output/challenge1b_output.json
```

**Expected output**: ~20 travel-relevant sections ranked by AI model based on semantic similarity.

## ğŸ”„ Complete Processing Pipeline

### 1. Input Validation & Configuration Loading
- Validates PDF files exist and are readable
- Parses `input_config.json` for persona and job requirements
- Initializes AI model with pre-trained weights

### 2. PDF Content Extraction
- Multi-layer PDF processing (PyMuPDF + PyPDF2 fallback)
- Preserves document structure and page references
- Handles various PDF formats and encodings

### 3. AI-Powered Semantic Analysis
- **Model Loading**: Loads e5-small-v2 transformer model (128MB)
- **Dynamic Query Generation**: AI creates persona-specific search context
- **Content Embedding**: Converts document sections to 384-dimensional vectors
- **Similarity Scoring**: Computes semantic relevance using cosine similarity

### 4. Intelligent Ranking & Selection
- Ranks all sections by AI-computed relevance scores
- Applies diversity filtering to avoid single-document clustering
- Selects top 20 most relevant sections with metadata

### 5. Adobe-Compliant Output Generation
- Formats results in exact Adobe specification
- Includes processing timestamp and metadata
- Validates output schema compliance

## ğŸ—ï¸ Project Structure

```
Round 1B Solution/
â”œâ”€â”€ ğŸ“„ README.md                    # This documentation
â”œâ”€â”€ ğŸ“„ approach_explanation.md      # Technical methodology (300-500 words)
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ³ Dockerfile                   # Optimized container build
â”œâ”€â”€ ğŸ main.py                      # Entry point
â”œâ”€â”€ ğŸ“ src/                         # Source code
â”‚   â”œâ”€â”€ persona_intelligence_e5.py  # Round 1B AI engine with e5-small-v2
â”‚   â””â”€â”€ process_pdfs.py             # PDF processing utilities
â”œâ”€â”€ ğŸ“ models/                      # Pre-downloaded e5-small-v2 (128MB)
â”œâ”€â”€ ğŸ“ wheels/                      # Pre-downloaded Python packages
â”œâ”€â”€ ğŸ“ input/                       # Sample test data
â”‚   â”œâ”€â”€ input_config.json          # Sample configuration
â”‚   â””â”€â”€ PDFs/                       # South of France travel documents
â”œâ”€â”€ ğŸ“ output/                      # Generated results
â””â”€â”€ ğŸ“ docs/                        # Technical documentation
```

## âš¡ Performance Specifications

| Metric | Adobe Requirement | Our Achievement | Status |
|--------|------------------|-----------------|--------|
| **Model Size** | â‰¤ 1GB | 128MB | âœ… **Excellent** |
| **Processing Time** | â‰¤ 60 seconds | < 10 seconds | âœ… **Excellent** |
| **Container Size** | Reasonable | 1.77GB | âœ… **Good** |
| **Offline Operation** | Required | Complete | âœ… **Perfect** |
| **Output Format** | Adobe-compliant | Exact match | âœ… **Perfect** |

## ğŸ”§ Technical Architecture

### AI Model Specifications
- **Model**: intfloat/e5-small-v2
- **Size**: 128MB (well under 1GB limit)
- **Dimensions**: 384
- **Language**: Multilingual support
- **Inference**: CPU-optimized, no GPU required
- **Processing**: Dynamic semantic understanding (no hardcoded rules)

### Semantic Processing Pipeline
1. **PDF Text Extraction**: High-fidelity text extraction with section identification
2. **Content Segmentation**: Intelligent chunking based on document structure
3. **AI Embedding Generation**: e5-small-v2 model creates 384-dimensional vectors
4. **Dynamic Relevance Scoring**: AI-computed cosine similarity against persona-job composite query
5. **Intelligent Ranking**: Top 20 most relevant sections with importance scores

## ğŸ› Troubleshooting

### Common Issues

**âŒ "Configuration file not found"**
- Ensure `input/input_config.json` exists
- Check file path and permissions
- Verify JSON format is valid

**âŒ "No PDF files found"**  
- Ensure PDFs are in `input/PDFs/` directory
- Check PDF filenames match those in config
- Verify PDF files are not corrupted

**âŒ "Model loading failed"**
- Increase Docker memory limit to 4GB+
- Restart Docker Desktop
- Rebuild image: `docker build --no-cache -t pdf-analyzer .`

**âŒ "Processing too slow"**
- Reduce number of PDFs
- Increase CPU allocation in Docker
- Check available system memory

### Debug Mode
Enable detailed logging:
```bash
docker run --rm -e DEBUG=true \
  -v "$(pwd)/input:/app/input" \
  -v "$(pwd)/output:/app/output" \
  pdf-analyzer
```

## ğŸ§¹ Development

### Local Development Setup
```bash
# Clone repository
git clone <repository-url>
cd AdobeHackathon

# Install dependencies
pip install -r requirements.txt

# Download model (for local testing)
python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('intfloat/e5-small-v2')"

# Run locally
python main.py --input ./input/PDFs --config ./input/input_config.json --output ./output
```

### Testing
```bash
# Quick functionality test
docker run --rm pdf-analyzer python -c "
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('./models/e5-small-v2')
print('âœ… Model loaded successfully')
"

# Full end-to-end test
docker run --rm \
  -v "$(pwd)/input:/app/input" \
  -v "$(pwd)/output:/app/output" \
  pdf-analyzer
```

## ğŸ“š Documentation

- **[approach_explanation.md](approach_explanation.md)**: Detailed methodology explanation
- **[docs/PROJECT_DOCUMENTATION.md](docs/PROJECT_DOCUMENTATION.md)**: Complete technical documentation
- **Source Code**: Fully documented Python modules in `src/`

## ğŸ† Adobe Hackathon 2025 Compliance

âœ… **Model Constraint**: 128MB << 1GB limit  
âœ… **Performance**: <10s << 60s limit  
âœ… **Output Format**: Perfect Adobe JSON compliance  
âœ… **Offline Operation**: Zero internet dependencies  
âœ… **Platform**: linux/amd64 Docker container  
âœ… **Reproducibility**: Deterministic results  
âœ… **AI-Powered**: No hardcoded persona rules - pure model-based intelligence

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

**ğŸš€ Ready for Adobe Hackathon 2025 Submission**

*Advanced AI-powered semantic document intelligence delivering persona-driven insights in seconds* âš¡
